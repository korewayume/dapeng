{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function, unicode_literals\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, UpSampling2D, Dropout, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json, os\n",
    "with open(\"u-net-train.json\") as cfg:\n",
    "    configuration = json.load(cfg)\n",
    "unet_model_path = configuration.get(\"unet_model_path\",\"ModelArchive/u-net-model.yaml\")\n",
    "unet_weight_path = configuration.get(\"unet_weight_path\",'ModelArchive/unet_best_checkpoint.hdf5')\n",
    "unet_weight_checkpoint_path = configuration.get(\"unet_weight_checkpoint_path\",'ModelArchive/unet_best_checkpoint.hdf5')\n",
    "tensorboard_log_directory = os.path.join(configuration[\"tensorboard_log_directory\"], datetime.now().strftime(\"%m-%d-%H-%M\"))\n",
    "load_weigth = configuration.get(\"\", )\n",
    "fit_verbose = configuration.get(\"fit_verbose\", True)\n",
    "mosaic_size = configuration.get(\"mosaic_size\", 256)\n",
    "learning_rate = configuration.get(\"learning_rate\", 1e-3)\n",
    "dropout_rate = configuration.get(\"dropout_rate\", 0.2)\n",
    "epochs = configuration.get(\"epochs\", 300)\n",
    "batch_size = configuration.get(\"batch_size\", 15)\n",
    "train_x_path = configuration[\"train_x_path\"]\n",
    "train_y_path = configuration[\"train_y_path\"]\n",
    "valid_x_path = configuration[\"valid_x_path\"]\n",
    "valid_y_path = configuration[\"valid_y_path\"]\n",
    "valid_batch_size = configuration.get(\"valid_batch_size\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_coefficient(output, target, axis=[1,2,3], smooth=1e-5):\n",
    "    \"\"\"Jaccard coefficient for comparing the similarity of two\n",
    "    batch of data, usually be used for binary image segmentation.\n",
    "    i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    output : tensor\n",
    "        A distribution with shape: [batch_size, ....], (any dimensions).\n",
    "    target : tensor\n",
    "        A distribution with shape: [batch_size, ....], (any dimensions).\n",
    "    axis : list of integer\n",
    "        All dimensions are reduced, default ``[1,2,3]``.\n",
    "    smooth : float\n",
    "        This small value will be added to the numerator and denominator.\n",
    "        If both output and target are empty, it makes sure dice is 1.\n",
    "        If either output or target are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``,\n",
    "        then if smooth is very small, dice close to 0 (even the image values lower than the threshold),\n",
    "        so in this case, higher smooth can have a higher dice.\n",
    "        \n",
    "    Examples\n",
    "    ---------\n",
    "    >>> a = tf.constant([[[[1],[0]],[[1],[1]]]],dtype=tf.float64)\n",
    "    >>> b = tf.constant([[[[1],[1]],[[1],[1]]]],dtype=tf.float64)\n",
    "    >>> val = jaccard_coefficient(a,b)\n",
    "    >>> session = tf.Session()\n",
    "    >>> session.run(val)\n",
    "\n",
    "    References\n",
    "    -----------\n",
    "    - `Jaccard coefficient <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
    "    \"\"\"\n",
    "    inse = K.sum(output * target, axis=axis)\n",
    "    l = K.sum(output * output, axis=axis)\n",
    "    r = K.sum(target * target, axis=axis)\n",
    "    dice = (2. * inse + smooth) / (l + r + smooth)\n",
    "    return K.mean(dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"unet-5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet(size):\n",
    "    inputs = Input(shape=(size, size, 3))\n",
    "    \n",
    "# left\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    \n",
    "    pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    \n",
    "    pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    \n",
    "    pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    \n",
    "    pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "\n",
    "# right\n",
    "    \n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
    "    up6 = Dropout(dropout_rate)(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
    "    up7 = Dropout(dropout_rate)(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
    "    up8 = Dropout(dropout_rate)(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
    "    up9 = Dropout(dropout_rate)(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics=[jaccard_coefficient])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_unet(mosaic_size)\n",
    "with open(unet_model_path, \"w+\") as mf:\n",
    "    mf.write(model.to_yaml())\n",
    "if load_weigth:\n",
    "    model.load_weights(unet_weight_path)\n",
    "model_checkpoint = ModelCheckpoint(unet_weight_checkpoint_path, monitor='loss', save_best_only=True, verbose=0)\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_log_directory, write_graph=True, write_images=True)\n",
    "model_callbacks = [model_checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_data = np.load(\"x_data.npy\")\n",
    "# y_data = np.load(\"y_data.npy\")\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=fit_verbose, shuffle=True,\n",
    "#           callbacks=model_callbacks, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.keras_tools import ImageDirectoryIterator, x_tif_reader, y_tif_reader\n",
    "import re\n",
    "filename_regex = re.compile(r\"^\\d{3}_\\d{3}_\\d{3}\\.tif\")\n",
    "print(\"正在扫描训练样本\")\n",
    "train_gen = ImageDirectoryIterator(train_x_path, train_y_path, x_tif_reader, y_tif_reader, filename_regex, batch_size=batch_size      )\n",
    "print(\"正在扫描验证样本\")\n",
    "valid_gen = ImageDirectoryIterator(valid_x_path, valid_y_path, x_tif_reader, y_tif_reader, filename_regex, batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"开始训练网络\")\n",
    "model.fit_generator(train_gen, steps_per_epoch=len(train_gen), validation_data=valid_gen, validation_steps=len(valid_gen),\n",
    "                    epochs=epochs, verbose=fit_verbose, callbacks=model_callbacks, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
